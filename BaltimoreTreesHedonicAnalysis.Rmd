---
title: "Baltimore Trees Hedonic Analysis"
author: "Angela E. Boag"
date: "February 11, 2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
r <- getOption("repos")
r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
options(repos = r)

if(!require(knitr)){
  install.packages("knitr")
}

if(!require(dplyr)){
  devtools::install_github("dplyr")
}

if(!require(corrplot)){
  devtools::install_github("corrplot")
}

if(!require(ggthemes)){
  devtools::install_github("ggthemes")
}

if(!require(ggplot2)){
  devtools::install_github("ggplot2")
}

if(!require(effects)){
  devtools::install_github("effects")
}

if(!require(sjPlot)){
  devtools::install_github("sjPlot")
}

if(!require(spdep)){
  devtools::install_github("spdep")
}

if(!require(jtools)){
  devtools::install_github("jtools")
}

if(!require(WVPlots)){
  devtools::install_github("WVPlots")
}

if(!require(moments)){
  devtools::install_github("moments")
}

if(!require(fastDummies)){
  devtools::install_github("fastDummies")
}

if(!require(tidyr)){
  devtools::install_github("tidyr")
}

if(!require(gridExtra)){
  devtools::install_github("gridExtra")
}

if(!require(car)){
  devtools::install_github("car")
}

if(!require(ape)){
  devtools::install_github("ape")
}

library('knitr')
library('ggplot2')
library('ggthemes') 
library('scales')
library('dplyr') 
library('tidyr')
library('data.table')
library('gridExtra')
library('corrplot')
library('plyr')
library('car')
library('sp')
library('nlme')
library('lme4')
library('spdep')
library('jtools')
library('WVPlots')
library('moments')
library('fastDummies')
library('sjPlot')
library('effects')
library('ape')
library('MASS')

```

## Tree Cover, Race, and House Prices in Baltimore, Maryland

This data analysis explores the variables predicting 2010 sale prices of 
townhome and single family homes in Baltimore, Maryland. 

Given that Baltimore neighborhoods are highly racially segregated, this analysis sought to determine if there is an interaction between the effects of census block group racial composition and tree canopy cover on home price. Our null hypothesis was that canopy cover contributes similarly to home sale price in neighborhoods with contrasting racial compositions.

```{r set workspace, echo=FALSE}
setwd("C:/Users/LabUser/Documents/Side_projects/BaltimoreTrees/Baltimore_Trees_R_Scripts/baltimore-trees-final")
```

We collected ecological and socioeconomic data from various sources. We obtained
LiDAR-derived canopy cover data at 30-m resolution for 2011 from https://daac.ornl.gov/CMS/guides/CMS_Maryland_AGB_Canopy.html (Dubayah et al. 2018). We interpolated a mean parcel canopy cover value for each property using bilinear interpolation (taking the mean of the 4 nearest cell centers). Given the 30-m resolution, this provides an estimate of canopy cover for tree cover on the home's property, as well as properties directly adjacent (immediate neighbors).

We obtained home sale price data from the State of Maryland Department of Planning (https://planning.maryland.gov/Pages/OurProducts/downloadFiles.aspx), and joined these data with more detailed home date (number of baths, square footage etc.) from the private property data management company SpecPrint (http://www.specprint.com/).

We obtained spatial data on parks, major roads, and other features from state and federal databases, and obtained block group-level census data as American Community Survey estimates (2013) from American FactFinder (https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml). We obtained neighborhood-level indices of property and violent crime from https://data.baltimorecity.gov/Neighborhoods/Crime-Safety-2010-2014-Shape/bywi-mtiu 

The full suite of predictor variables include:

* Canopy Cover (CanCov30) within 30-m
* CanCov100
* CanCov400 
* lnAcres (Natural log of property acreage)
* YrBuilt (Year Built)
* lnSqFt (Natural log of home square feet)
* Bath (Number of bathrooms)
* Condition (Factor: Poor, Fair, Average, Good, Excellent)
* HousingType (Factor: Single family vs. Townhome)
* Viol11 (Neighborhood violent crime index 2011)
* DistDT (Distance to Downtown Baltimore)
* DistRoad (Distance to major roads) 
* DistParks (Distance to parks and arboretums)
* PercHS (Percent high school graduates (Block Group))
* lnMedHHInc (Median Household Income (Block Group))
* PercOwnOcc (Percent owner-occupied homes (Block Group)) 
* MedAge (Median age (Block Group))
* PercB (percent African American (Block Group-level))
* PercW (percent white (Block Group-level))

```{r}
## Loading, cleaning and prepping data
setwd("C:/Users/LabUser/Documents/Side_projects/BaltimoreTrees/Baltimore_Trees_R_Scripts/baltimore-trees-final")
home_sale <- read.csv("all_homes_clean_final.csv")
str(home_sale)

# Rename response variable and change to numeric
home_sale$Price <- home_sale$ConsideredPrice_spec
home_sale$Price <- as.numeric(home_sale$Price)
# hist(home_sale$Price)

# histogram of home values 

price_hist <- ggplot(home_sale, aes(x = Price)) +
  geom_histogram(binwidth = 15000, color = "black", fill = "white") +
  labs(x = "Sale Price (USD) 2010", y = "Count") + 
  scale_x_continuous(labels=dollar_format(prefix="$")) +
  theme_classic()

price_hist

summary(home_sale$Price)

ggsave("sale_price_histogram.png")


# Drop unnecessary variables
str(home_sale)
home_sale <- subset(home_sale, select = -c(ConsideredPrice_spec, 
                                           SaleMonth, SaleSeason, ACCTID_BaCi, 
                                           ConsideredPrice_BaCi, 
                                           csa2010, ACCTID_BaCi.1, cleanID2 
                                            ))
head(home_sale)


```

```{r}
# The sale price distribution looks skewed; testing skewness
skewness(home_sale$Price)

# It is right-skewed skewed, suggesting a natural log transformation may be 
# appropriate. But, we used a Box-Cox transformation of home_sale to justify 
# the log-transform (see code chunk where base model is fit)

# Transform response to natural log of sales price
home_sale$lnPrice <- log(home_sale$Price)
hist(home_sale$lnPrice)
skewness(home_sale$lnPrice) # Distribution much more normal now

# Change data types of other variables as needed

home_sale$Condition <- ordered(home_sale$Condition, 
                        c("POOR", "FAIR", "AVERAGE", "GOOD", "EXCELLENT"))
plot(lnPrice ~ Condition, data = home_sale)

# Identify missing data
any(is.na(home_sale))
# summary(home_sale)
# str(home_sale)

```

## Exploratory Data Analysis

```{r echo = FALSE, eval = FALSE}
# Identifying differences between home condition classes

library(dplyr)
head(home_sale)
group_by(home_sale, Condition) %>%
  summarise(
    count = n(),
    mean = mean(Price),
    sd = sd(Price)
  )
 
summary(home_sale$Condition)

# There are so few "Excellents" that we should combine them with GOOD and 
# recode them to GOOD_EX

home_sale$Condition2[home_sale$Condition=="GOOD"] <- "GOODEX"
home_sale$Condition2[home_sale$Condition=="EXCELLENT"] <- "GOODEX"
home_sale$Condition2[home_sale$Condition=="POOR"] <- "POOR"
home_sale$Condition2[home_sale$Condition=="FAIR"] <- "FAIR"
home_sale$Condition2[home_sale$Condition=="AVERAGE"] <- "AVERAGE"

# Convert the column to a factor and specify ordered vs. unordered factor
# Note: If you specify an ordered factor, R will with a linear, quadratic, 
# cubic etc. for each sequential level (and show up in the regression output as
# levels with .L, .Q, and .C etc.)
home_sale$Condition2 <- factor(home_sale$Condition2, levels = c("POOR", "FAIR", 
                                                      "AVERAGE", "GOODEX"), 
                                                      ordered = FALSE)
summary(home_sale$Condition2)

# Compute the analysis of variance
cond.aov <- aov(lnPrice ~ Condition2, data = home_sale)
cond.aov
# Summary of the analysis shows sig. diff. between groups.
summary(cond.aov)
# Look at pairwise differences, and it appears they're all sig. diff. from 
# each other. Keep in all condition levels for now.
TukeyHSD(cond.aov)

# check ANOVA assumptions
# 1. Homogeneity of variances (HOV)
plot(cond.aov, 1)
leveneTest(log(Price) ~ Condition, data = home_sale)
# Levene's test fails to reject null hypoth. of HOV

# 2. Normally distributed residuals (normality)
plot(cond.aov, 2)
# Extract the residuals
cond_residuals <- residuals(object = cond.aov )
# Run Shapiro-Wilk test
shapiro.test(x = cond_residuals)
# plot looked a little wonky and Shapiro-Wilk norm. test indicates non-normal
# residual distribution

# Non-parametric Kruskal-Wallis rank sum test
kruskal.test(lnPrice ~ Condition2, data = home_sale)
# again confirms sig. differences between groups
```

```{r}

# Distributions of numeric variables
str(home_sale)
home_sale_numeric <- subset(home_sale, select = -c(Condition,
                                                   Condition2, 
                                                    HousingType, ZipName
                                                   ))
numeric_var_hists <- home_sale_numeric %>%
                      gather() %>% 
                      ggplot(aes(value)) +
                      facet_wrap(~ key, scales = "free") +
                      geom_histogram()
numeric_var_hists

# Summary statistics for predictor variables
df_sum <- home_sale_numeric %>%
  summarise_each(list(min = min,
                      median = median, 
                      mean = mean,
                      max = max,
                      sd = sd))
head(df_sum)

df_sum_tidy <- df_sum %>% 
  gather(stat, val) %>%
  separate(stat, into = c("var", "stat"), sep = "_") %>%
             spread(stat, val) %>%
             select(var, min, median, mean, max, sd)
print(df_sum_tidy)
  

# After multiple iterations of data exploration, modeling, and checking 
# assumptions (as shown in the workflow below), I determined I should natural 
# log-transform Median HH Income, Acres and SqFt because of skew that violates 
# linear regression modeling assumptions of linear relationships between 
# predictors and response.

home_sale$lnAcres <- log(home_sale$Acres)
hist(home_sale$lnAcres, breaks = 20)

home_sale$lnSqFt <- log(home_sale$SqFt)
hist(home_sale$lnSqFt, breaks = 20)

home_sale$lnMedHHInc <- log(home_sale$MedHHInc)
hist(home_sale$lnMedHHInc, breaks = 20)

# I will also try log-tranformed versions of the proximity variables.
home_sale$lnDistDT <- log(home_sale$DistDT)
hist(home_sale$lnDistDT, breaks = 20)

home_sale$lnDistRoad <- log(home_sale$DistRoad)
hist(home_sale$lnDistRoad, breaks = 20)

home_sale$lnDistParks <- log(home_sale$DistParks)
hist(home_sale$lnDistParks, breaks = 20)

# Counts of factor variables

home_sale_factor <- subset(home_sale, select = c(SaleSeason, Condition, 
                                                 HousingType, ZipName
                                                 ))

apply(home_sale_factor, 2, table)

# Making "donut" percent cover estimates

# Comparing correlations between different canopy cover measures
cancov <- subset(home_sale, select = c(CanCovPerc, CanCov100, CanCov200,
                                       CanCov400
                                                 ))
cor(cancov)

cancov <- subset(home_sale, select = c(CanCovPerc_bilin, CanCov100, CanCov200, CanCov400, cancov_sum30, cancov_sum100, cancov_sum30_100, 
                                       cancov_sum400, cancov_sum100_400
                                                 ))
cor(cancov)
# all of the canopy cover variables are highly co-linear; the lowest
# correlation is between cancov_sum30 and cancov_sum100_400 (i.e. canopy
# cover within a 30-m radius and total canopy cover within a 100-400m radius)

# I ultimately left Sale Season out of the analysis because it was not
# significant in any models.

# Potential issue: If we use ZipName as a random effect, it may be 
# problematic to have a Zip with only 1 sale (Catonsville)

# Let's look at collinearity between variables to identify potentially
# important/problematic correlations.
str(home_sale)

home_sale_numeric2 <- subset(home_sale, select = -c(Condition, Condition2, 
                                                    HousingType, ZipName, Price
                                                   ))
corr_home_sale <- round(cor(home_sale_numeric2, method = c("spearman")), 2)
corr_home_sale
corrplot(corr_home_sale, method = c("ellipse"))

# How to read this correlation plot: Blue indicates positive correlation, red
# indicates negative correlation. The narrower the elipses, the stronger the
# correlation.

```

Looking at the correlation matrix and plot, it's clear that Baltimore is highly 
segregated due to the very high negative correlation between percent black and
percent white in Census block groups. We'll proceed with the analysis using just
one of these variables, percent black. Other potentially problematic high correlations between variables include crime indices Prop11 and Viol11, which makes sense. There seems to be almost no correlation between property crimes and home price, so I'm going to proceed only using violent crime. LnAcres and DistDT are highly correlated (0.74), so I'm going to keep an eye on the Variance Inflation Factors (VIFs) for these variables. 



# More Exploratory Analyses

```{r}

# Look at variable relationships
neighborhoods <- ggplot(home_sale, aes(x = reorder(ZipName, Price, FUN = mean),
                                       y = Price)) + geom_boxplot() + 
  labs(x = "Zip Code Name", y = "Sale Price (USD)") + 
  scale_y_continuous(labels=dollar_format(prefix="$")) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
neighborhoods

# Note on neighborhoods: Mt. Washington and Roland Park have fairly high mean 
# sale prices compared to other neighborhoods. Using ZipName as a random effect
# may account for this.

```

```{r}

# Relationship between lnPrice and predictor variables

# str(home_sale)

# The code below produces a multipanel plot with all predictors by response, but it's a little busy, so I'm going with individual plots.

# home_sale %>%
#  gather(-lnPrice, -ZipName, -Price, -Lat, -Lon, -PercW, -Acres, -SqFt, -MedHHInc,
#         key = "var", value = "value") %>%
#  ggplot(aes(x = value, y = lnPrice)) +
#    geom_point(alpha = 0.3) +
#    facet_wrap(~ var, scales = "free") +
#    theme_bw()

plot(lnPrice ~ lnAcres, data = home_sale)
plot(lnPrice ~ YrBuilt, data = home_sale)
plot(lnPrice ~ lnSqFt, data = home_sale)
plot(lnPrice ~ Bath, data = home_sale)
plot(lnPrice ~ Viol11, data = home_sale)
plot(lnPrice ~ cancov_sum30, data = home_sale)
plot(lnPrice ~ cancov_sum30_100, data = home_sale)
plot(lnPrice ~ cancov_sum100_400, data = home_sale)
plot(lnPrice ~ DistDT, data = home_sale)
plot(lnPrice ~ DistRoad, data = home_sale)
plot(lnPrice ~ DistParks, data = home_sale)
plot(lnPrice ~ PercHS, data = home_sale)
plot(lnPrice ~ lnMedHHInc, data = home_sale)
plot(lnPrice ~ PercOwnOcc, data = home_sale)
plot(lnPrice ~ MedAge, data = home_sale)
plot(lnPrice ~ PercB, data = home_sale)

# Relationship between main effects

plot(CanCovPerc ~ PercB, data = home_sale)

# I'm noticing a couple data cleaning issues that need to be addressed. There
# are three data points nearly 3km from a park -- is this right?
# Also, there are three datapoints with 0 perc HS grads, which is wrong. Same
# with HHInc, PercOwnOcc, medianAge (also see one over 80 -- what?)
# View these weird points

# zeros <- subset(home_sale, PercHS == 0)
# zeros

# UPDATE: After looking at these incorrect values, I imputed values for cleanID 1471 because it was on the border of Morrell Park and therefore didn't have census data populated, but I filled it in from houses in the same block group. I deleted cleanID 739 because it fell outside the borders of the block groups, as well as cleanID 975 because it was in a non-residentially zoned neighborhood.

# The relationship between home prices and YrBuilt as well as lnAcres is a bit U-shaped, but
# not sure if it justifies a quadratic form. I'll test this in the
# regressions.

```

## Fit multiple linear regression models

Our modeling strategy had four main components:

1. Initially, we fit a multiple linear regression model to predict lnPrice from the main effects Canopy Cover (CanCovPerc) and percent African American (Block Group-level) (PercB), in addition to the following control variables:

* lnAcres (Natural log of property acreage); and square of lnAcres
* YrBuilt (Year Built); and square of YrBuilt
* lnSqFt (Natural log of home square feet)
* Bath (Number of bathrooms)
* Condition (Factor: Poor, Fair, Average, Good, Excellent)
* HousingType (Factor: Single family vs. Townhome)
* Viol11 (Neighborhood violent crime index 2011)
* DistDT (Distance to Downtown Baltimore)
* DistRoad (Distance to major roads) 
* DistParks (Distance to parks and arboretums)
* PercHS (Percent high school graduates (Block Group))
* lnMedHHInc (Median Household Income (Block Group))
* PercOwnOcc (Percent owner-occupied homes (Block Group)) 
* MedAge (Median age (Block Group))

This initial model did not include any variable interactions. We evaluated whether the model met assumptions and tested for residual spatial autocorrelation.

2. Upon detecting significant residual spatial autocorrelation, we fit a random effects model with random intercepts based on ZipName (Zip code). Exploratory analyses indicated higher sale prices in some Zip Codes than others. We again tested for residual spatial autocorrelation.

3. Upon detecting no significant residual spatial autocorrelation in the mixed effects model, we tested  the performance of a model including an interaction between Canopy Cover (CanCovPerc) and percentage black residents in a census block group (PercB). In addition to evaluating whether this interaction was significant, we also compared model performance to the model without interactions based on minimizing RMSE.

4. For all models, we tested model performance by fitting the models on 75% of the dataframe (the training set) and predicting the remaining 25% (the test set).

```{r}
# Revise this chunk to simplify the dataset and scale variables as necessary 
# Drop other unnecessary variables (in preliminary regressions SaleSeason had
# no predictive ability, so I'm dropping it).

# Center and standardize all numeric variables before regression modeling.
dim(home_sale)
str(home_sale)
cols <- c(2:5, 9:30, 34:39)
home_sale[, cols] <- lapply(home_sale[, cols], function(x) {
  y<-scale(x, center=TRUE, scale=TRUE)
  }
)
# summary(home_sale)

any(is.na(home_sale))
# test
```


```{r fit initial model and use BoxCox}

# Fit initial MLRs testing different canopy cover radii 

head(home_sale)


home_model1 <- lm(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition2 + HousingType + Viol11 + cancov_sum30 + 
                        lnDistDT + lnDistRoad + lnDistParks + PercHS + lnMedHHInc +
                        PercOwnOcc + MedAge + PercB, data = home_sale)

summary(home_model1)

home_model1b <- lm(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition2 + HousingType + Viol11 + cancov_sum30_100 + 
                        lnDistDT + lnDistRoad + lnDistParks + PercHS + lnMedHHInc +
                        PercOwnOcc + MedAge + PercB, data = home_sale)

summary(home_model1b)

home_model1c <- lm(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition2 + HousingType + Viol11 + cancov_sum100_400 + 
                        lnDistDT + lnDistRoad + lnDistParks + PercHS + lnMedHHInc +
                        PercOwnOcc + MedAge + PercB, data = home_sale)

summary(home_model1c)


# Checking assumptions
plot(home_model1)
# cook <- cooks.distance(home_model1)

# Residuals look OK. Some heterosckedasticity with less variance at higher values. 

# Let's try a box-cox transformation. Make sure Price var is not scaled.
summary(home_sale$Price)
b <- boxcox(Price ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition2 + HousingType + Viol11 + CanCovPerc + 
                        DistDT + DistRoad + DistParks + PercHS + lnMedHHInc +
                        PercOwnOcc + MedAge + PercB, data = home_sale)
lambda <- b$x # lambda values

lik <- b$y # log likelihood values for SSE

bc <- cbind(lambda, lik) # combine lambda and lik

sorted_bc <- bc[order(-lik),] # values are sorted to identify the lambda value for the maximum log likelihood for obtaining minimum SSE

head(sorted_bc, n = 10)
# top Box-Cox value was 0.22, suggesting log transformation (lamda = 0) is 
# appropriate.


# Looking at outliers.
# (home_sale[875,]) # This house is said to have sold for $1,049,000 in 2010, but it's a 3-bd, 1 bath townhouse with an estimate of $122,976. I think this sale price was entered incorrectly, and should be $104,900. Three other houses in the same census block all sold in 2010 for around $100,000, so I'm going to make the change [CHANGE MADE].
# (home_sale[648,]) 
# This is simply the lowest sale price, but the house characteristics make sense for such a low value. The next highest values are in the $30k range, so I think I'll leave it in.

# Try model without outliers
# home_sale <- home_sale[-c(875), ]

# Model predictions based on observations
home_sale$prediction <-  predict(home_model1, home_sale)

# Make a plot to compare predictions to observed (prediction on x axis)
ggplot(home_sale, aes(x = prediction, y = lnPrice)) +
	geom_point() +
	geom_abline(color = "blue")
# Predictions look pretty good.

# Attach residuals to dataframe
home_sale$residuals <- home_sale$prediction - home_sale$lnPrice
# str(home_sale)

# Calcute RMSE
err <- home_sale$prediction - home_sale$lnPrice

#Square the error vector
err2 <- err^2

#Take the mean, and sqrt it to get the RMSE:
(rmse <- sqrt(mean(err2)))

# Now try same model with squared YrBuilt
home_model2 <- lm(lnPrice ~ lnAcres + I(YrBuilt^2) + lnSqFt + Bath 
                        + Condition + HousingType + Viol11 + CanCovPerc + DistDT
                        + DistRoad + DistParks + PercHS + lnMedHHInc 
                        + PercOwnOcc + MedAge + PercB, data = home_sale)

summary(home_model2)
# Yr_Built isn't significant in either form (though is on 0.05 cusp), so I think I'll leave it as-is. 

# Now try same model with squared LnAcres
home_model3 <- lm(lnPrice ~ I(lnAcres^2) + YrBuilt + lnSqFt + Bath 
                        + Condition + HousingType + Viol11 + CanCovPerc + DistDT
                        + DistRoad + DistParks + PercHS + lnMedHHInc 
                        + PercOwnOcc + MedAge + PercB, data = home_sale)

summary(home_model3)
# lnAcres is significant in both forms so we'll include the squared term. 

# Is model3 better than model1? Comparing the RMSEs (simply changed model number
# in RMSE calculation above) indicates they aren't that different, and 
# model1 is actually slightly better (lower RMSE). 

```



```{r}
# Looking at where the model performs better/worse

GainCurvePlot(home_sale, "prediction", "lnPrice", "home_model1")
# Wow this actually looks really, really good.

# CREATE TRAINING AND TESTING SETS

# Use nrow to get the number of rows in mpg (N) and print it

(N <- nrow(home_sale))

# Calculate how many rows 75% of N should be and print it
# Hint: use round() to get an integer

(target <- round(N * 0.75))


# Create the vector of N uniform random variables: gp
gp <- runif(N)


# Use gp to create the training set: mpg_train (75% of data) and mpg_test 
# (25% of data)

home_sale_train <- home_sale[gp < 0.75, ]

home_sale_test <- home_sale[gp >= 0.75, ]


# Use nrow() to examine mpg_train and mpg_test

nrow(home_sale_train)

nrow(home_sale_test)

# Fit model on training data
home_model1_traintest <- lm(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition + HousingType + Viol11 + CanCovPerc + DistDT
                        + DistRoad + DistParks + PercHS + lnMedHHInc 
                        + PercOwnOcc + MedAge + PercB, 
                            data = home_sale_train)


# Make predictions on home_sale_test

home_sale_test$prediction <- predict(home_model1_traintest, newdata = 
                                       home_sale_test)

#  Plot predicted income (x axis) vs home_sale_test$lnPrice

ggplot(home_sale_test, aes(x = prediction, y = lnPrice)) + 
	geom_point() + 
	geom_abline(color = "blue")

# Again, looks pretty good.

# Mapping the residuals.
qplot(Lon, Lat, color = residuals, data = home_sale)
# Doesn't look like much clustering is going on, but let's test for 
# residual spatial autocorrelation using Moran's I.

```


## Test for Spatial Autocorrelation

```{r}
# I'm also going to fit the same model using the nlme package so I can look
# at the semivariogram
home_model_nlme <- gls(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition + HousingType + Viol11 + CanCovPerc + DistDT
                        + DistRoad + DistParks + PercHS + lnMedHHInc 
                        + PercOwnOcc + MedAge + PercB, data = home_sale)

# summary(home_model_nlme)

# Import data as a spatial dataframe (spdf)

home_sale_sp <- home_sale
coordinates(home_sale_sp) <- ~Lon+Lat
# home_sale_sp

homeCRS <- CRS("+proj=longlat +init=epsg:4269")

proj4string(home_sale_sp) <- CRS("+proj=longlat +init=epsg:4269")

# plot(home_sale_sp, pch = 16)

# str(home_sale_sp)

# summary(home_sale_sp$residuals)

# semivariogram

semivario1 <- Variogram(home_model_nlme, form = ~Lon+Lat, resType = "normalized")
plot(semivario1, smooth = TRUE)

# Doesn't really look like there's a lag based on the semivariogram

### Moran's I: Calculate Euclidean distance matrix ###

# I initially had an error here but I figured out the problem. I needed to jitter my lat/lon locations by a tiny bit for those few Townhomes that have virtually identical Lat/Lon coordinates in order to compute a distance matrix. I did this in the CSV file by adding radomly generated numbers to the end decimal place.

geo <- cbind(home_sale$Lon, home_sale$Lat)
dist_mat <- as.matrix(dist(geo)) # Euclidean distance
# dist_mat_inv <- 1/dist_mat # inverse distance matrix
# diag(dist_mat_inv) <- 0
# summary(dist_mat_inv)
Moran.I(residuals(home_model_nlme), dist_mat, alternative = "two.sided")
```

The Moran's I test indicates significant residual spatial autocorrelation (SAC) (p < 0.05), despite the unsuspicious variogram. Let's fit a mixed effects model to try and account for the residual SAC.

## Linear Mixed-Effects Model

```{r}

# Linear mixed-effects model

home_model_random1 <- lme(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition2 + HousingType + Viol11 + CanCovPerc 
                        + DistDT + DistRoad + DistParks + PercHS + lnMedHHInc 
                        + PercOwnOcc + MedAge + PercB, random = ~1|ZipName, 
                        method = "ML",  data = home_sale)

printCoefmat(summary(home_model_random1)$tTable)

# Checking assumptions.
plot(home_model_random1)
qqnorm(residuals(home_model_random1, type = "normalized"))
abline(0,1, col = "red", lty = 2)

# plot the semivariogram
semivario2 <- Variogram(home_model_random1, form = ~Lon+Lat, resType = "normalized")
plot(semivario2, smooth = TRUE)

# Test residual SAC with Moran's I
Moran.I(residuals(home_model_random1), dist_mat, alternative = "two.sided")
# p > 0.05, so no residual spatial autocorrelation. Great! Now let's test interactions.

# Note: While I have written code for evaluating these mixed effects models  
# (i.e. fitting on 75% of the data, testing on 25%; checking further 
# assumptions), those code chunks are omitted here for brevity, and with the 
# recognition that these models may change depending on feedback.

```

```{r}
# Comparing mixed effects models with/without interactions

home_model_random2 <- lme(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath  
                          + Condition + HousingType + Viol11 
                          + PercB*CanCovPerc + DistDT + DistRoad + DistParks 
                          + PercHS + lnMedHHInc + PercOwnOcc + MedAge, 
                          random = ~1|ZipName, method = "ML", home_sale)

printCoefmat(summary(home_model_random2)$tTable)

# It appears that the interaction between Canopy Cover and percent black residents is not a significant predictor of lnPrice. Let's inspect the interaction.

# The standard deviation of percent canopy cover should just be 1, because we
# centered and scaled by SD all variables.
# To verify, you would use sd(home_sale$variable)

ef1 <- effect(term="PercB*CanCovPerc", home_model_random2, xlevels =  list(PercB = c(-1, 0, 1), CanCovPerc = c(-1, 0, 1)))

efdata1 <- as.data.frame(ef1) # convert the effects list to a data frame
efdata1 # print effects data frame

# Create factors of the different variables in your interaction 
efdata1$PercB <- factor(efdata1$PercB,
                      levels=c(-1, 0, 1),
                      labels=c("1 SD Below Mean", "Mean", "1 SD Above Mean"))
                     
efdata1$CanCovPerc <- factor(efdata1$CanCovPerc,
              levels=c(-1, 0, 1),
              labels=c("Low Cover", "Average Cover", "High Cover"))


ggplot(efdata1, aes(x=CanCovPerc, y=fit, color = PercB, group = PercB)) + 
    geom_point(aes(color = PercB)) + 
    geom_line(size=1.2, aes(color = PercB)) +
    geom_ribbon(aes(ymin=fit-se, ymax=fit+se, fill = PercB),alpha=0.3) + 
    labs(x= "Canopy Cover", y="lnPrice") + 
      scale_color_manual(values=c("#99CCFF","#9999FF","#6666CC")) +
      scale_fill_manual(values = c("#99CCFF","#9999FF","#6666CC")) +
    theme_bw() + 
  theme(text=element_text(size=14))
#custom color coding 

```

This interaction plot shows that higher canopy cover is associated with higher home sale price, and that the slope of this effect is relatively consistent between neighborhoods with a low proportion of black residents, mean proportion of black residents, and a high proportion of black residents. This is consistent with our null hypothesis that the positive relationship between canopy cover and home sale price does not vary by the racial composition of Baltimore neighborhoods. 

```{r}
# Exploratory analyses indicated a potential interaction between violent crime and canopy cover, so we'll explore that interaction too.

home_model_random3 <- lme(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath  
                          + Condition + HousingType + PercB
                          + Viol11*CanCovPerc + DistDT + DistRoad + DistParks 
                          + PercHS + lnMedHHInc + PercOwnOcc + MedAge, 
                          random = ~1|ZipName, method = "ML", home_sale)

printCoefmat(summary(home_model_random3)$tTable)

ef2 <- effect(term="Viol11*CanCovPerc", home_model_random3, xlevels =  list(Viol11 = c(-1, 0, 1), CanCovPerc = c(-1, 0, 1)))

efdata2 <- as.data.frame(ef2) # convert the effects list to a data frame
efdata2 # print effects data frame

# Create factors of the different variables in your interaction 
efdata2$Viol11 <- factor(efdata2$Viol11,
                      levels=c(-1, 0, 1),
                      labels=c("1 SD Below Mean", "Mean", "1 SD Above Mean"))
                     
efdata2$CanCovPerc <- factor(efdata2$CanCovPerc,
              levels=c(-1, 0, 1),
              labels=c("Low Cover", "Average Cover", "High Cover"))


ggplot(efdata2, aes(x=CanCovPerc, y=fit, color = Viol11, group = Viol11)) + 
    geom_point(aes(color = Viol11)) + 
    geom_line(size=1.2, aes(color = Viol11)) +
    geom_ribbon(aes(ymin=fit-se, ymax=fit+se, fill = Viol11),alpha=0.3) + 
    labs(x= "Canopy Cover", y="lnPrice") + 
      scale_color_manual(values=c("#CCCCCC","#FFFF00","#FF0000")) +
      scale_fill_manual(values = c("#CCCCCC","#FFFF00","#FF0000")) +
    theme_bw() + 
  theme(text=element_text(size=14))
#custom color coding 
```
The mixed-effects model and the interaction plot indicate no significant interaction between the effects of canopy cover and violent crime levels on home prices (i.e., higher canopy cover is associated with higher home sale value in safe neighborhoods as well as high-violent-crime neighborhoods).

```{r}
# And what about a 3-way interaction between Canopy Cover, race and crime?
home_model_random4 <- lme(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath  
                          + Condition + HousingType + PercB*Viol11*CanCovPerc 
                          + DistDT + DistRoad + DistParks 
                          + PercHS + lnMedHHInc + PercOwnOcc + MedAge, 
                          random = ~1|ZipName, method = "ML", home_sale)

printCoefmat(summary(home_model_random4)$tTable)

# It appears that there is a significant interaction between the effects of
# proportion black residents and violent crime on home price.

# ranef(home_sale_random5)

ef3 <- effect(term="PercB*Viol11", home_model_random4, xlevels =list(Viol11 = c(-1, 0, 1), PercB = c(-1, 0, 1)))

efdata3 <- as.data.frame(ef3) # convert the effects list to a data frame
efdata3 # print effects data frame

# Create factors of the different variables in your interaction 
efdata3$Viol11 <- factor(efdata3$Viol11,
                      levels=c(-1, 0, 1),
                      labels=c("1 SD Below Mean", "Mean", "1 SD Above Mean"))
                     
efdata3$PercB <- factor(efdata3$PercB,
              levels=c(-1, 0, 1),
              labels=c("Low Proportion", "Average Proportion", "High Proportion"))


ggplot(efdata3, aes(x=PercB, y=fit, color = Viol11, group = Viol11)) + 
    geom_point(aes(color = Viol11)) + 
    geom_line(size=1.2, aes(color = Viol11)) +
    geom_ribbon(aes(ymin=fit-se, ymax=fit+se, fill = Viol11),alpha=0.3) + 
    labs(x= "Proportion black residents", y="lnPrice") + 
      scale_color_manual(values=c("#FFCC99","#CC0066","#660033")) +
      scale_fill_manual(values = c("#FFCC99","#CC0066","#660033")) +
    theme_bw() + 
  theme(text=element_text(size=14))
```

This interaction is not related to canopy cover, but it is an important interaction impacting sale price. Home sale prices are lower in neighborhoods with a higher proportion of black residents in general, but even moreso for neighborhoods with high rates of violent crime.

## Comparing Models

```{r}
# Compare mixed effects models via AIC and log likelihood.
anova(home_model_random1, home_model_random2, home_model_random3, home_model_random4)
```

Model 4, which incorporates the interaction between violent crime and proportion black residents, performs the best, and has higher explanatory power than the models including interactions between race, crime, and canopy cover. 

```{r, eval=FALSE, echo = FALSE}

# interaction plots
interact_plot(home_model_random3, pred = "CanCovPerc", modx = "PercB")

str(home_sale$Condition) # examines intercept of random effects
plot(home_sale$Viol11 ~ home_sale$PercB)

# ASSUMPTIONS
# 1. Testing linearity -- residuals look a little wierd. **dig into this.
plot.model.linearity <- plot(resid(home_sale_random4), home_sale$lnPrice)

# 2. Homog. variances from https://ademos.people.uic.edu/Chapter18.html
home_sale$mod_res<- residuals(home_sale_random4) #extracts the residuals and places them in a new column in our original data table
home_sale$abs_mod_res <-abs(home_sale$mod_res) #creates a new column with the absolute value of the residuals
home_sale$mod_res2 <- home_sale$abs_mod_res^2 #squares the absolute values of the residuals to provide the more robust estimate
Levene_model <- lm(mod_res2 ~ ZipName, data = home_sale) #ANOVA of the squared residuals
anova(Levene_model) #displays the results
# Residuals DO NOT appear to have equal variances. ***How can we deal with this?
plot(home_sale_random4) # Looks like variances are smaller at high sale prices

# 3. Residuals are normally distributed. 
require("lattice")
qqmath(home_sale_random4) ## quantile-quantile

# Fit model on training data
home_model_random3_traintest <- lmer(lnPrice ~ SaleSeason + Acres + YrBuilt + SqFt + Bath 
                        + Condition + HousingType + PercB*Viol11*CanCovPerc + DistDT 
                        + DistRoad + DistParks + PercHS + MedHHInc + PercOwnOcc 
                        + MedAge + (1| ZipName), data = home_sale_train)


# Make predictions on home_sale_test

home_sale_test$prediction <- predict(home_model_random3_traintest, newdata = 
                                       home_sale_test)

#  Plot predicted income (x axis) vs home_sale_test$lnPrice

ggplot(home_sale_test, aes(x = prediction, y = lnPrice)) + 
	geom_point() + 
	geom_abline(color = "blue")

vcov(home_sale_random4)

# Check model linearity for each independent variable

ggplot(data.frame(x1 = home_sale$MedAge,pearson=residuals(home_sale_random4, type="pearson")),
      aes(x=x1, y=pearson)) +
    geom_point() +
    theme_bw()

plot(lnPrice ~ Acres, data = home_sale)

hist(home_sale$SqFt)

```

# Findings (thus far)

Our analyses demonstrate that higher canopy cover is associated with 
higher sale prices of homes sold in the city of Baltimore in 2010, regardless of the racial composition of the block group a home is located in.

This means that buyers in both predominantly white and predominantly black
neighborhoods percieve value in urban tree canopy. Given that there is wide 
variation in the levels of canopy cover across Baltimore neighborhoods, there are opportunities to enhance home values in all neighborhood types through tree planting and urban greening. These findings may also contribute to critical discussions of green gentrification. 

```{r, eval = FALSE, echo = FALSE}

home_model4 <- lm(lnPrice ~ lnAcres + YrBuilt + lnSqFt + Bath 
                        + Condition + HousingType + CanCovPerc*PercB + Viol11 +  DistDT + DistRoad + DistParks + PercHS + lnMedHHInc 
                        + PercOwnOcc + MedAge, data = home_sale)

summary(home_model4)

interact_plot(home_model4, pred = "PercB", modx = "CanCovPerc")

home_model3 <- lm(lnPrice ~ CanCovPerc*Acres + I(YrBuilt^2) + SqFt + Bath 
                        + Condition + HousingType + Viol11*PercB*CanCovPerc 
                        + DistDT + DistRoad + DistParks + PercHS + CanCovPerc*MedHHInc 
                        + CanCovPerc*PercOwnOcc + CanCovPerc*MedAge, data = home_sale)

summary(home_model3)
interact_plot(home_model3, pred = "Viol11", modx = "PercB")
interact_plot(home_model3, pred = "CanCovPerc", modx = "PercB")
interact_plot(home_model3, pred = "Viol11", modx = "CanCovPerc")

plot(PercB ~ CanCovPerc, data = home_sale)
plot(CanCovPerc ~ Viol11, data = home_sale)

```
